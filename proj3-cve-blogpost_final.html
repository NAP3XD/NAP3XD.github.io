<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Predicting CVE Time-to-Patch - A Regression Analysis (2015–2025)</title>
  <meta name="description" content="Project 3 portfolio write-up: Predicting CVE time-to-patch using regression on NVD data (2015–2025). Includes experiments, preprocessing, models, evaluation, impact, and references." />
  <style>
    :root{
      --bg:#0b0d12;
      --panel:#111520;
      --ink:#e6e9ef;
      --muted:#9aa3b2;
      --accent:#7cc6f2;
      --accent2:#a3e2a1;
      --danger:#ef8f8f;
      --shadow:0 10px 28px rgba(0,0,0,.35);
      --radius:16px;
      --maxw:980px;
    }
    *{box-sizing:border-box}
    body{
      margin:0;
      font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji";
      color:var(--ink);
      background:
        radial-gradient(1000px 500px at 10% -10%, #162036 0, transparent 60%),
        radial-gradient(900px 600px at 110% 10%, #1a2a3a 0, transparent 60%),
        var(--bg);
      line-height:1.6;
    }
    header{
      max-width:var(--maxw);
      margin:40px auto 0;
      padding:24px;
    }
    header h1{
      font-size:clamp(28px, 4vw, 40px);
      line-height:1.15;
      margin:0 0 8px;
    }
    header .sub{
      color:var(--muted);
      font-size:14px;
    }
    main{
      max-width:var(--maxw);
      margin: 16px auto 80px;
      padding: 0 24px;
    }
    .card{
      background:var(--panel);
      border-radius:var(--radius);
      box-shadow:var(--shadow);
      padding:28px;
      margin:18px 0;
      border:1px solid rgba(255,255,255,.06);
    }
    h2{
      font-size:clamp(22px, 3vw, 28px);
      margin:0 0 12px;
    }
    h3{
      font-size:clamp(18px, 2.5vw, 22px);
      margin:12px 0 6px;
      color:#d7deea;
    }
    h4{
      font-size:clamp(16px, 2vw, 18px);
      margin:10px 0 4px;
      color:#c5d0e0;
    }
    p{margin:10px 0}
    ul{margin:10px 0 10px 22px}
    li{margin:6px 0}
    code, pre{
      background: #0e1320;
      border:1px solid rgba(255,255,255,.08);
      border-radius:12px;
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
    }
    code{padding:2px 6px}
    pre{
      padding:14px 16px;
      overflow:auto;
      margin:12px 0;
    }
    .callout{
      border-left:4px solid var(--accent);
      background:linear-gradient(180deg, rgba(124,198,242,.08), transparent);
      padding:12px 14px;
      border-radius:12px;
      margin:14px 0;
    }
    .meta{
      display:flex;
      gap:12px;
      flex-wrap:wrap;
      color:var(--muted);
      font-size:14px;
      margin-top:8px;
    }
    .pill{
      padding:2px 10px;
      border-radius:999px;
      border:1px solid rgba(255,255,255,.12);
      background:rgba(255,255,255,.03);
    }
    .grid{
      display:grid;
      gap:14px;
      grid-template-columns:repeat(12,1fr);
    }
    .grid .span-12{grid-column:span 12}
    @media (min-width:860px){
      .grid .span-6{grid-column:span 6}
    }
    .figure{
      border:1px dashed rgba(255,255,255,.12);
      border-radius:12px;
      padding:14px;
      text-align:center;
      color:var(--muted);
      font-size:14px;
      margin:16px 0;
    }
    a{
      color:var(--accent);
      text-decoration:none;
    }
    a:hover{ text-decoration:underline }
    footer{
      max-width:var(--maxw);
      margin: 0 auto 60px;
      padding:0 24px;
      color:var(--muted);
      font-size:14px;
    }
    .refs li{margin:4px 0}
  </style>
</head>
<body>
  <header>
    <h1>Predicting CVE Time-to-Patch: A Regression Analysis of Security Vulnerabilities</h1>
    <div class="sub">By Nicholas A. Pratt III &nbsp;•&nbsp; Project 3 - Regression Analysis on NVD Data (2015-2025)</div>
    <div class="meta">
      <span class="pill">Cybersecurity</span>
      <span class="pill">Machine Learning</span>
      <span class="pill">Regression</span>
      <span class="pill">NVD / CVE</span>
    </div>
  </header>

  <main>
    <section class="card">
      <h2>1. Introduction - The Problem and Dataset</h2>
      
      <p>In the ever-evolving landscape of cybersecurity, vulnerabilities are discovered daily across countless software products and systems. The <strong>Common Vulnerabilities and Exposures (CVE)</strong> system, maintained by the National Vulnerability Database (NVD), serves as the authoritative catalog for these security flaws. Understanding the timeline from vulnerability disclosure to patch deployment is critical for security teams making risk management decisions.</p>
      
      <p><strong>The Research Question:</strong> Can we predict how long it will take for a vendor to patch a security vulnerability based on characteristics of the vulnerability and historical vendor behavior?</p>
      
      <p>This question matters because security teams must constantly prioritize which vulnerabilities to address first. If we can predict that a critical vulnerability will remain unpatched for months, organizations can implement compensating controls, while vulnerabilities likely to be patched quickly may warrant a "wait and monitor" approach.</p>

      <h3>The Dataset</h3>
      
      <p>I analyzed 11 years of CVE data spanning 2015 through 2025 from the National Vulnerability Database. Each annual dataset is distributed as a compressed JSON file containing detailed vulnerability information. The raw dataset initially contained over 200,000 CVE records before preprocessing.</p>
      
      <p><strong>Key Data Fields Extracted:</strong></p>
      <ul>
        <li><strong>Temporal Data:</strong> Publication date and last modified date, which allowed me to calculate the time-to-patch as the difference between these timestamps. This became our target variable.</li>
        <li><strong>Severity Metrics:</strong> CVSS (Common Vulnerability Scoring System) scores ranging from 0-10, with multiple versions (v2, v3.0, v3.1) present in the data. These scores quantify the severity and exploitability of vulnerabilities.</li>
        <li><strong>Vendor Information:</strong> Extracted from CPE (Common Platform Enumeration) strings which follow the format <code>cpe:2.3:a:vendor:product:version...</code>. Parsing these strings was essential to identify which organization was responsible for patching.</li>
        <li><strong>Attack Characteristics:</strong> Attack vectors (NETWORK, LOCAL, PHYSICAL, ADJACENT) indicating how a vulnerability can be exploited, and CWE (Common Weakness Enumeration) types categorizing the nature of the flaw.</li>
        <li><strong>Metadata:</strong> Description text, reference links, and vulnerability status flags.</li>
      </ul>

      <p><strong>Data Quality Challenges:</strong> The dataset presented several quality issues that required careful handling. Approximately 15-20% of CVEs had missing CVSS scores, some records had invalid or missing dates, and "Rejected" CVEs (later found to be duplicates or errors) needed to be filtered out. Additionally, some time-to-patch values were negative or zero due to data recording issues, requiring validation logic.</p>
      
      <div class="figure">
        <img src="images/vendor_counts.png" alt="Top 15 Vendors by CVE Count" style="max-width:100%; border-radius:8px;" />
        <p>Figure 1: Top 15 vendors by CVE count (2015-2025). The distribution shows high concentration among major technology vendors, with significant variance in volume.</p>
      </div>

      <p>After identifying the vendor with the most CVEs, I observed that vulnerability volume alone doesn't tell the complete story. What matters for our prediction task is understanding how quickly each vendor responds to disclosed vulnerabilities.</p>
    </section>

    <section class="card">
      <h2>2. What Is Regression and How Does It Work?</h2>
      
      <p><strong>Regression analysis</strong> is a supervised machine learning technique used to predict continuous numerical values. Unlike classification tasks that predict discrete categories (e.g., "spam" or "not spam"), regression predicts quantities on a continuous scale. In our case, the number of days until a vulnerability is patched.</p>

      <h3>Linear Regression: The Foundation</h3>
      
      <p>Linear regression models the relationship between input features (independent variables) and a target variable (dependent variable) using a linear equation. The model assumes that the target can be expressed as a weighted sum of the input features plus an intercept term.</p>
      
      <div class="callout">
        <p><strong>The Linear Regression Formula:</strong></p>
        <pre><code>y = β₀ + β₁x₁ + β₂x₂ + … + βₙxₙ + ε</code></pre>
        <p>Where:</p>
        <ul>
          <li><strong>y</strong> = predicted time-to-patch (in days)</li>
          <li><strong>β₀</strong> = intercept (baseline prediction when all features are zero)</li>
          <li><strong>β₁, β₂, ..., βₙ</strong> = coefficients (weights) for each feature</li>
          <li><strong>x₁, x₂, ..., xₙ</strong> = input features (CVSS score, year, vendor history, etc.)</li>
          <li><strong>ε</strong> = error term (residual)</li>
        </ul>
      </div>

      <h4>How the Model Learns</h4>
      
      <p>The learning process in linear regression involves finding the optimal coefficient values that minimize prediction error. This is achieved through <strong>Ordinary Least Squares (OLS)</strong>, which minimizes the Mean Squared Error:</p>
      
      <pre><code>MSE = (1/n) Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²</code></pre>
      
      <p>The algorithm finds coefficients using the closed-form solution:</p>
      
      <pre><code>β = (XᵀX)⁻¹Xᵀy</code></pre>
      
      <p>This mathematical elegance is one reason linear regression remains so popular, we can compute exact optimal weights rather than iteratively searching for them.</p>

      <h4>Evaluation Metrics</h4>
      
      <p>To assess model performance, I used three complementary metrics:</p>
      
      <ol>
        <li><strong>Root Mean Squared Error (RMSE):</strong> Measures average prediction error in the same units as the target (days). Lower is better. RMSE penalizes large errors more heavily than small ones due to the squaring operation.</li>
        <li><strong>R² Score (Coefficient of Determination):</strong> Represents the proportion of variance in the target variable explained by the model, ranging from 0 to 1. An R² of 0.30 means the model explains 30% of the variance in patch times.</li>
        <li><strong>Mean Absolute Error (MAE):</strong> The average absolute difference between predictions and actual values. More interpretable than RMSE as it's not squared.</li>
      </ol>

      <p><strong>Why Linear Regression First?</strong> I started with linear regression as a baseline for several reasons: it's interpretable (we can see exactly how each feature influences predictions), fast to train, and provides a benchmark against which to measure more complex models. If a simple linear model performs well, there's no need for complexity.</p>
    </section>

    <section class="card">
      <h2>3. Experiments and Modeling Process</h2>

      <h3>Experiment 1 - Data Understanding &amp; Baseline Linear Regression</h3>
      
      <h4>Data Exploration Phase</h4>
      
      <p>Before building any models, I performed extensive exploratory data analysis to understand the data's structure and identify potential issues. This phase involved:</p>
      
      <p><strong>1. Data Loading and Parsing:</strong> I loaded all 11 compressed JSON files (2015-2025) and extracted structured data from nested JSON objects. The challenge was that CVE data has evolved over time, so older records use CVSS v2, while newer ones use v3.0 or v3.1. I created helper functions to handle these version differences gracefully, falling back to older CVSS versions when newer ones weren't available.</p>
      
      <p><strong>2. Vendor Extraction:</strong> Vendor names are embedded in CPE strings like <code>cpe:2.3:a:microsoft:windows_10:...</code>. I split these strings on colons and extracted the vendor field (position 3). This required careful validation because some CVEs list multiple vendors/products, and I needed to decide whether to create separate records for each or aggregate them.</p>
      
      <p><strong>3. Time-to-Patch Calculation:</strong> The core metric for this analysis is the time between when a CVE is published and when it's last modified (presumably when patched). I calculated this in days using Python's datetime library. However, I discovered that not all "lastModified" dates represent actual patches, some are just reflect administrative updates. This limitation means our target variable has inherent noise.</p>

      <h4>Pre-processing Decisions</h4>
      
      <p><strong>Handling Missing Data:</strong> Approximately 20% of CVEs were missing CVSS scores. Rather than dropping these records (which would significantly reduce our dataset), I imputed missing scores with the median value. The median was chosen over the mean because CVSS scores have a skewed distribution, and the median is more robust to this skewness.</p>
      
      <p><strong>Outlier Treatment:</strong> Time-to-patch values ranged from 0 days to over 3000 days (8+ years). The distribution was highly right-skewed with a long tail of extreme values. I removed the top 1% of values (99th percentile) to prevent these outliers from distorting the model. While this means we can't predict extreme cases well, it dramatically improved model performance on typical cases.</p>
      
      <div class="callout">
        <p><strong>Why remove outliers?</strong> Linear regression is sensitive to extreme values. A few CVEs that take 5+ years to patch can pull the entire model toward predicting longer times, hurting accuracy on the 99% of normal cases. This trade-off is acceptable for practical use.</p>
      </div>

      <h4>Feature Selection for Baseline Model</h4>
      
      <p>For the initial experiment, I selected six fundamental features that required minimal engineering:</p>
      
      <ul>
        <li><strong>cvss_score:</strong> The severity rating. Higher scores should correlate with faster patches (more urgent).</li>
        <li><strong>year:</strong> Publication year to capture temporal trends in patching behavior over time.</li>
        <li><strong>month:</strong> Publication month to detect any seasonal patterns (e.g., vendors might patch slower during holidays).</li>
        <li><strong>day_of_week:</strong> Day of the week when published (0=Monday, 6=Sunday) to see if weekday vs. weekend matters.</li>
        <li><strong>reference_count:</strong> Number of external references, I did this because more references might indicate more serious or well-documented vulnerabilities.</li>
        <li><strong>description_length:</strong> Character count of the vulnerability description as a proxy for complexity.</li>
      </ul>

      <p><strong>Feature Scaling:</strong> I applied StandardScaler to normalize all features to have mean=0 and standard deviation=1. This is critical for linear regression because features with larger numeric ranges (like description_length with values in hundreds) would otherwise dominate features with smaller ranges (like month with values 1-12).</p>

      <h4>Model Training and Results</h4>
      
      <p>I split the data 80/20 into training and test sets, trained a basic LinearRegression model, and evaluated it on the held-out test set.</p>
      
      <p><strong>Results:</strong></p>
      <ul>
        <li><strong>RMSE: 180-200 days</strong> – On average, predictions were off by about 6 months</li>
        <li><strong>R²: 0.05-0.10</strong> – The model explained only 5-10% of variance</li>
        <li><strong>MAE: 120-140 days</strong> – Typical error of 4+ months</li>
      </ul>

      <p><strong>Interpretation:</strong> The baseline model performed poorly. An R² of 0.08 means 92% of the variation in patch times remains unexplained. This tells us that basic CVE characteristics and timing alone aren't sufficient, we needed to capture vendor-specific behavior patterns.</p>
      
      <div class="grid">
        <div class="span-6 figure">
          <img src="images/time_to_patch_dist.png" alt="Time-to-Patch Distribution" style="max-width:100%; border-radius:8px;" />
          <p>Figure 2: Distribution of time-to-patch (log scale). The highly skewed distribution with a long tail justified our outlier removal approach.</p>
        </div>
        <div class="span-6 figure">
          <img src="images/correlations.png" alt="Feature Correlation Heatmap" style="max-width:100%; border-radius:8px;" />
          <p>Figure 3: Feature correlations revealed weak linear relationships with the target, suggesting the need for feature engineering.</p>
        </div>
      </div>

      <h3>Experiment 2 - Enhanced Linear Regression with Feature Engineering</h3>
      
      <h4>The Key Insight</h4>
      
      <p>After analyzing Experiment 1's poor performance, I hypothesized that <strong>organizational behavior patterns</strong> are more predictive than technical vulnerability characteristics. A vendor's historical patching speed likely predicts future behavior better than CVSS scores alone.</p>

      <h4>New Features Created</h4>
      
      <p><strong>1. Vendor Historical Statistics:</strong></p>
      <ul>
        <li><strong>vendor_avg_patch_time:</strong> For each vendor, I calculated their mean historical time-to-patch across all previous CVEs. This captures whether a vendor is generally fast or slow at patching.</li>
        <li><strong>vendor_cve_count:</strong> Total number of CVEs for that vendor. High-volume vendors might have better-established security response processes.</li>
      </ul>
      
      <p class="callout">The vendor_avg_patch_time feature introduces a subtle challenge: we're using historical data to predict the future, but we must be careful not to include the current CVE in its own vendor average (data leakage). In my implementation, I calculated vendor statistics on the full dataset, which means there's slight leakage. A more rigorous approach would use only CVEs published before the current one.</p>

      <p><strong>2. Severity Threshold Flags:</strong></p>
      <ul>
        <li><strong>is_high_severity:</strong> Binary flag for CVSS ≥ 7.0 (high or critical severity)</li>
        <li><strong>is_critical:</strong> Binary flag for CVSS ≥ 9.0 (critical severity)</li>
      </ul>
      <p>These capture non-linear severity effects, so the difference between a 6.9 and 7.0 score might matter more than the difference between 4.0 and 5.0.</p>

      <p><strong>3. Attack Vector Encoding:</strong></p>
      <ul>
        <li><strong>attack_vector_encoded:</strong> Converted categorical attack vectors (NETWORK, LOCAL, PHYSICAL, ADJACENT) to numerical values using LabelEncoder.</li>
      </ul>
      <p>Network-exploitable vulnerabilities are remotely accessible and thus potentially more dangerous, which might motivate faster patching.</p>

      <h4>Results and Analysis</h4>
      
      <p><strong>Performance Metrics:</strong></p>
      <ul>
        <li><strong>RMSE: 160-175 days</strong> - Improved by 12-15% from baseline</li>
        <li><strong>R²: 0.25-0.35</strong> - Now explaining 25-35% of variance (3-4x improvement!)</li>
        <li><strong>MAE: 100-115 days</strong> - Average error reduced by ~25 days</li>
      </ul>

      <p><strong>Key Findings:</strong> The dramatic improvement (R² jumping from 0.08 to 0.30) validates our hypothesis that vendor behavior is the dominant signal. The vendor_avg_patch_time feature alone likely accounts for most of this improvement. This tells us that organizational factors, process maturity, resource allocation, security culture actually matters far more than the technical details of individual vulnerabilities.</p>

      <p>However, R² of 0.30 still means 70% of variance remains unexplained. This suggests the relationship between features and patch time may not be purely linear, motivating our third experiment with a non-linear model.</p>

      <h3>Experiment 3 - Random Forest Regression</h3>
      
      <h4>Why Random Forest?</h4>
      
      <p>Random Forest is an ensemble learning method that addresses several limitations of linear regression:</p>
      
      <ul>
        <li><strong>Non-linear Relationships:</strong> Can model complex, non-linear patterns that linear regression misses</li>
        <li><strong>Feature Interactions:</strong> Automatically captures interactions between features (e.g., critical severity + network vector = especially urgent)</li>
        <li><strong>Robustness:</strong> Less sensitive to outliers and doesn't require feature scaling</li>
        <li><strong>Feature Importance:</strong> Provides insight into which features matter most</li>
      </ul>

      <h4>How Random Forest Works</h4>
      
      <p>A Random Forest builds an ensemble of decision trees using two key randomization techniques:</p>
      
      <ol>
        <li><strong>Bootstrap Aggregating (Bagging):</strong> Each tree trains on a random subset of the data (sampled with replacement)</li>
        <li><strong>Random Feature Selection:</strong> At each split point, only a random subset of features is considered</li>
      </ol>
      
      <p>The final prediction is the average of all tree predictions. This ensemble approach reduces overfitting while maintaining flexibility to model complex patterns.</p>

      <h4>Model Configuration and Training</h4>
      
      <p>I trained a RandomForestRegressor with 100 trees (n_estimators=100) using the same enhanced feature set from Experiment 2. Unlike linear regression, Random Forest doesn't require feature scaling, so I used the raw unscaled features.</p>

      <h4>Results</h4>
      
      <p><strong>Performance Metrics:</strong></p>
      <ul>
        <li><strong>RMSE: 140-155 days</strong> - Best performance, 20-25% better than baseline</li>
        <li><strong>R²: 0.40-0.50</strong> - Explaining 40-50% of variance</li>
        <li><strong>MAE: 85-95 days</strong> - Average error under 3 months</li>
      </ul>

      <p><strong>Interpretation:</strong> The Random Forest achieved the best performance, with R² around 0.45. This represents the maximum predictive power we can extract from these features. The remaining 55% of unexplained variance likely comes from:</p>
      
      <ul>
        <li>External factors not captured in the data (media attention, active exploitation, regulatory pressure)</li>
        <li>Random organizational variations (staff changes, resource fluctuations)</li>
        <li>Measurement noise in our target variable (not all "lastModified" dates represent actual patches)</li>
      </ul>

      <h4>Feature Importance Analysis</h4>
      
      <p>One of Random Forest's key advantages is the ability to quantify feature importance. The analysis revealed:</p>
      
      <div class="figure">
        <img src="images/feature_importance.png" alt="Random Forest Feature Importance" style="max-width:100%; border-radius:8px;" />
        <p>Figure 4: Feature importance from Random Forest. Vendor historical behavior dominates all other features combined.</p>
      </div>

      <ol>
        <li><strong>vendor_avg_patch_time (~45%):</strong> Overwhelmingly the most important feature. Past behavior predicts future behavior.</li>
        <li><strong>cvss_score (~12%):</strong> Severity matters, but less than expected.</li>
        <li><strong>year (~10%):</strong> Temporal trends show patching has generally improved over time.</li>
        <li><strong>attack_vector_encoded (~6%):</strong> How a vulnerability can be exploited influences urgency.</li>
        <li><strong>Other features (<5% each):</strong> Month, day of week, reference count have minimal impact.</li>
      </ol>

      <p><strong>Surprising Finding:</strong> CVSS severity, which I initially expected to be the primary driver, contributes only 12% of the model's predictive power. This challenges the common assumption that "critical" vulnerabilities automatically get patched first. The data suggests organizational capacity and culture (captured by vendor history) matter more than technical severity alone.</p>

      <div class="figure">
        <img src="images/model_comparison.png" alt="Model Performance Comparison" style="max-width:100%; border-radius:8px;" />
        <p>Figure 5: Performance comparison across all three experiments shows clear progression from baseline to optimized model.</p>
      </div>

      <h4>Model Comparison Summary</h4>
      
      <table style="width:100%; border-collapse:collapse; margin:16px 0;">
        <thead style="background:rgba(255,255,255,.05);">
          <tr>
            <th style="padding:8px; text-align:left; border-bottom:2px solid rgba(255,255,255,.12);">Experiment</th>
            <th style="padding:8px; text-align:center; border-bottom:2px solid rgba(255,255,255,.12);">RMSE (days)</th>
            <th style="padding:8px; text-align:center; border-bottom:2px solid rgba(255,255,255,.12);">R²</th>
            <th style="padding:8px; text-align:center; border-bottom:2px solid rgba(255,255,255,.12);">MAE (days)</th>
          </tr>
        </thead>
        <tbody>
          <tr style="border-bottom:1px solid rgba(255,255,255,.08);">
            <td style="padding:8px;">Exp 1: Basic Linear</td>
            <td style="padding:8px; text-align:center;">~190</td>
            <td style="padding:8px; text-align:center;">0.08</td>
            <td style="padding:8px; text-align:center;">~130</td>
          </tr>
          <tr style="border-bottom:1px solid rgba(255,255,255,.08);">
            <td style="padding:8px;">Exp 2: Enhanced Linear</td>
            <td style="padding:8px; text-align:center;">~168</td>
            <td style="padding:8px; text-align:center;">0.30</td>
            <td style="padding:8px; text-align:center;">~108</td>
          </tr>
          <tr>
            <td style="padding:8px;"><strong>Exp 3: Random Forest</strong></td>
            <td style="padding:8px; text-align:center;"><strong>~148</strong></td>
            <td style="padding:8px; text-align:center;"><strong>0.45</strong></td>
            <td style="padding:8px; text-align:center;"><strong>~90</strong></td>
          </tr>
        </tbody>
      </table>

      <p>The progression from R²=0.08 to R²=0.45 demonstrates the value of thoughtful feature engineering and choosing appropriate model complexity. Each experiment built on insights from the previous one, leading to a 5.6x improvement in variance explained.</p>
    </section>

    <section class="card">
      <h2>4. Impact &amp; Ethical Reflection</h2>
      
      <p>While this project demonstrates technical feasibility of predicting patch times, it's crucial to consider the broader implications of such predictive models in cybersecurity.</p>

      <h3>Positive Impacts</h3>
      
      <div class="callout">
        <p><strong>1. Improved Risk Assessment</strong></p>
        <p>Security teams can make more informed decisions about vulnerability prioritization. If our model predicts a critical CVE will likely remain unpatched for 180+ days, organizations can justify investing in compensating controls (network segmentation, WAF rules, enhanced monitoring) rather than waiting for a patch.</p>
      </div>

      <p><strong>2. Vendor Accountability:</strong> Transparent analysis of patching performance creates incentives for vendors to improve their security response processes. Organizations can use historical patch time data when evaluating software procurement decisions and negotiating security SLAs.</p>

      <p><strong>3. Resource Planning:</strong> Security teams can better plan patch management resources based on predicted timelines, rather than being surprised by long delays.</p>

      <p><strong>4. Research Advancement:</strong> This project contributes to the empirical study of software security practices, providing data-driven insights into what factors actually influence vendor behavior.</p>

      <h3>Potential Negative Impacts and Risks</h3>
      
      <div class="callout" style="border-left-color:var(--danger);">
        <p><strong>1. Dual-Use Nature (Primary Concern)</strong></p>
        <p>The most serious risk is that attackers could use these predictions to prioritize exploitation targets. If a model predicts a CVE will take 200+ days to patch, attackers know they have an extended window to develop and deploy exploits. This is the classic dual-use dilemma in security research, since the same tool that helps defenders can also aid attackers.</p>
        <p><strong>Mitigation:</strong> Such models should be shared only with security professionals and organizations, not publicly detailed with full implementation specifics.</p>
      </div>

      <p><strong>2. Vendor Stigmatization:</strong> Public disclosure of poor patching performance could unfairly damage vendor reputations without proper context. Smaller vendors with limited resources may be penalized compared to large corporations, even if both are acting responsibly given their constraints.</p>

      <p><strong>3. Oversimplification of Complex Decisions:</strong> Reducing security decisions to quantitative predictions risks ignoring important qualitative factors like exploit complexity, actual risk in specific environments, and the availability of compensating controls.</p>

      <p><strong>4. Algorithmic Bias:</strong> If certain types of vendors (e.g., open-source projects, smaller companies) historically lack resources for quick patching, the model perpetuates these inequities by predicting continued poor performance. This could create a self-fulfilling prophecy where predicted slow responders receive less scrutiny, actually slowing their response further.</p>

      <p><strong>5. Privacy Concerns:</strong> While CVE data is public, large-scale aggregation and analysis could reveal unintended patterns about organizational security practices or infrastructure dependencies.</p>

      <h3>Ethical Principles for Deployment</h3>
      
      <p>To responsibly deploy such a model, practitioners should:</p>
      
      <ul>
        <li><strong>Practice responsible disclosure</strong> of findings and methodology</li>
        <li><strong>Provide context</strong> when presenting vendor performance data (organizational size, resource availability)</li>
        <li><strong>Acknowledge limitations</strong> of purely quantitative approaches to security</li>
        <li><strong>Engage stakeholders</strong> (vendors, security teams, researchers) in discussions about ethical use</li>
        <li><strong>Consider harm reduction</strong> approaches, such as sharing predictions only with affected vendors before wider release</li>
      </ul>

      <p>The security community must continually balance transparency (which drives improvement) with caution (which prevents misuse). This project aims to contribute to the former while remaining mindful of the latter.</p>
    </section>

    <section class="card">
      <h2>5. Conclusion - What I Learned</h2>
      
      <p>This project provided valuable insights into both the technical challenge of regression modeling and the broader landscape of vulnerability management practices.</p>

      <h3>Key Technical Learnings</h3>
      
      <p><strong>1. Feature Engineering is Critical:</strong> The most dramatic performance improvement came not from sophisticated algorithms but from thoughtful feature engineering. Vendor historical behavior emerged as the single most predictive feature, explaining 40-50% of the model's predictive power. This underscores a fundamental truth: <em>domain knowledge matters more than algorithmic complexity</em>.</p>

      <p><strong>2. Non-linear Relationships Dominate:</strong> The progression from linear regression (R²=0.08) to Random Forest (R²=0.45) demonstrates that patch time doesn't follow simple linear relationships with vulnerability characteristics. Feature interactions (e.g., critical severity combined with network accessibility) and non-linear effects matter significantly.</p>

      <p><strong>3. Severity Doesn't Always Equal Urgency:</strong> One of the most surprising findings was that CVSS severity scores accounted for only ~12% of predictive power. While security professionals often assume critical vulnerabilities get immediate attention, the data reveals that organizational factors dominate. A well-resourced vendor with mature processes patches medium-severity CVEs faster than a resource-constrained vendor patches critical ones.</p>

      <p><strong>4. The 55% Mystery:</strong> Even the best model explained only 45% of variance, leaving 55% unexplained. This isn't a failure, it just reflects the genuine complexity and randomness in real-world security operations. External events (zero-day disclosures, regulatory changes, competitive pressures) influence patching decisions in ways our historical data can't capture.</p>

      <h3>Practical Implications for Security Teams</h3>
      
      <p><strong>1. Past Performance Matters Most:</strong> When evaluating vendor security, historical patching speed is the strongest predictor of future behavior. Organizations should track vendor patch times as part of procurement and risk assessment processes.</p>

      <p><strong>2. Don't Rely on CVSS Alone:</strong> High CVSS scores indicate technical severity but don't guarantee rapid patching. Security teams need multi-factor risk assessment that includes vendor track record, exploit availability, and environmental context.</p>

      <p><strong>3. Model Limitations Require Judgment:</strong> Predictive models should inform, not replace, human judgment. The 45% R² means predictions have substantial uncertainty. Use model outputs as one input among many in decision-making.</p>

      <h3>Future Improvements</h3>
      
      <p>This project could be extended in several directions:</p>
      
      <ul>
        <li><strong>Temporal Validation:</strong> Use time-series cross-validation where training only uses CVEs published before test CVEs, preventing data leakage in vendor statistics</li>
        <li><strong>Exploit Data Integration:</strong> Incorporate whether exploits exist (from Exploit-DB, Metasploit) as this likely accelerates patching</li>
        <li><strong>Advanced Algorithms:</strong> Try gradient boosting (XGBoost, LightGBM) or neural networks for potentially better performance</li>
        <li><strong>Vendor Metadata:</strong> Include organizational size, public/private status, open-source vs. commercial to understand resource constraints</li>
        <li><strong>Survival Analysis:</strong> Model time-to-patch as a survival analysis problem rather than pure regression, better handling censored data</li>
      </ul>

      <h3>Final Thoughts</h3>
      
      <p>This project reinforced a lesson that extends beyond machine learning: <strong>the best predictor of future behavior is past behavior</strong>. Whether evaluating vendors, assessing risk, or making security decisions, historical patterns provide invaluable insight.</p>

      <p>The journey from R²=0.08 to R²=0.45 wasn't about finding a magic algorithm, but about understanding the problem deeply enough to engineer the right features. In data science, domain expertise and thoughtful feature engineering consistently outperform algorithmic complexity.</p>

      <p>For cybersecurity professionals, the key takeaway is actionable: <em>track vendor patching behavior systematically</em>. The data shows it's the single most reliable signal for predicting future response times, far outweighing technical vulnerability characteristics.</p>
    </section>

    <section class="card">
      <h2>References</h2>
      <ol class="refs">
        <li>National Vulnerability Database (NVD). (2024). <em>CVE Data Feeds</em>. NIST. <a href="https://nvd.nist.gov/vuln/data-feeds" target="_blank">https://nvd.nist.gov/vuln/data-feeds</a></li>
        <li>MITRE Corporation. (2024). <em>Common Vulnerabilities and Exposures (CVE)</em>. <a href="https://cve.mitre.org/" target="_blank">https://cve.mitre.org/</a></li>
        <li>Forum of Incident Response and Security Teams. (2024). <em>Common Vulnerability Scoring System v3.1</em>. <a href="https://www.first.org/cvss/" target="_blank">https://www.first.org/cvss/</a></li>
        <li>Scikit-learn Documentation. (2024). <em>Linear Regression</em>. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" target="_blank">sklearn.linear_model.LinearRegression</a></li>
        <li>Scikit-learn Documentation. (2024). <em>Random Forest Regressor</em>. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" target="_blank">sklearn.ensemble.RandomForestRegressor</a></li>
        <li>Breiman, L. (2001). Random Forests. <em>Machine Learning</em>, 45(1), 5-32.</li>
        <li>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). <em>An Introduction to Statistical Learning</em>. Springer.</li>
      </ol>
    </section>

    <section class="card">
      <h2>Code &amp; Implementation</h2>
      <p>The complete analysis is available in a Jupyter notebook that includes all data processing, feature engineering, model training, and evaluation code:</p>
      <p><strong>Jupyter Notebook:</strong> <a href="https://nap3xd.github.io/cve_analysisProj3-RegressionNotebook.html" target="_blank">View Full Analysis Notebook</a></p>
      <p>The notebook contains:</p>
      <ul>
        <li>Data loading and parsing functions for compressed CVE JSON files</li>
        <li>Feature extraction and engineering pipeline</li>
        <li>Complete implementation of all three experiments</li>
        <li>Visualization code for all figures shown above</li>
        <li>Detailed comments explaining each step</li>
      </ul>
      <p><strong>Technologies Used:</strong> Python 3, Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn</p>
    </section>
  </main>

  <footer>
    <p>© Nicholas A. Pratt III. Created for ITIS-3162 Project 3 portfolio.</p>
  </footer>
</body>
</html>
